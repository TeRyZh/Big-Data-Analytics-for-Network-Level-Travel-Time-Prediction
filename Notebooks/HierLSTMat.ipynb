{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5K9dv5UXFWma"
      },
      "outputs": [],
      "source": [
        "!apt install cuda\n",
        "!pip install pytorch_forecasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2tgOOg7VkBR",
        "outputId": "d2f91a1c-75b7-4709-96f2-811a05b0c18f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrtkcH47Fa3P"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFQDsVyVFN-H"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import zipfile\n",
        "import time\n",
        "import itertools\n",
        "\n",
        "import matplotlib\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "import numpy as np\n",
        "\n",
        "plt.style.use('seaborn')\n",
        "import random\n",
        "\n",
        "import math\n",
        "import pdb\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from pytorch_forecasting.metrics import MAE, RMSE, MAPE\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "def try_gpu():\n",
        "    \"\"\"If GPU is available, return mx.gpu(0); else return mx.cpu().\"\"\"\n",
        "    try:\n",
        "        ctx = mx.gpu()\n",
        "        _ = nd.array([0], ctx=ctx)\n",
        "    except:\n",
        "        ctx = mx.cpu()\n",
        "    return ctx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuNQFJRgFRRd",
        "outputId": "91e8d6f6-7863-4dd2-b739-86c13dd4b49c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Sep 15 08:22:38 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    36W / 250W |    747MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkQfDbbjfGzb"
      },
      "outputs": [],
      "source": [
        "travel_time_df = pd.read_csv('/content/drive/MyDrive/CIS 545 Project Folder/Data Set/2021_travel_time.csv')\n",
        "corridor_file = \"/content/drive/MyDrive/CIS 545 Project Folder/Data Set/pems_district4_corridors.csv\"\n",
        "corridor_df = pd.read_csv(corridor_file)\n",
        "\n",
        "travel_time_df = travel_time_df[travel_time_df['5 Minutes'] > '2021-01-01 00:00:00']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJwqybR6fI52"
      },
      "outputs": [],
      "source": [
        "corridors_to_merge = []\n",
        "\n",
        "for row_num, corridor in corridor_df.iterrows():  \t\n",
        "      if row_num == 0:\n",
        "          all_corridors_tt_2021 = travel_time_df[(travel_time_df['corridor_name'] == corridor[\"Corridor\"])\t&  (travel_time_df['direction'] == corridor[\"Fwy-Dir\"])][[\"5 Minutes\",\t\"Mainline Agg\"]]\n",
        "      else:\n",
        "          corridor_tt_2021 = travel_time_df[(travel_time_df['corridor_name'] == corridor[\"Corridor\"])\t&  (travel_time_df['direction'] == corridor[\"Fwy-Dir\"])][[\"5 Minutes\",\t\"Mainline Agg\"]]\n",
        "          if corridor_tt_2021['5 Minutes'].duplicated().any() :  # to avoid duplicates cause by primary and secondary route\n",
        "            print(corridor[\"Corridor\"], \" \", corridor[\"Fwy-Dir\"])\n",
        "            continue\n",
        "          all_corridors_tt_2021 = all_corridors_tt_2021.merge(corridor_tt_2021, \"outer\", left_on='5 Minutes', right_on='5 Minutes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kro7BO9bfLKj"
      },
      "outputs": [],
      "source": [
        "# all_corridors_tt_2021"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhACAbfifNGD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        },
        "outputId": "f0b190f1-353a-45a1-a690-9de4e49a0014"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 5 Minutes  Mainline Agg_x  Mainline Agg_y  Mainline Agg_y  \\\n",
              "0      2021-01-27 00:00:00           10.95           11.57           10.92   \n",
              "1      2021-01-27 00:05:00           11.00           11.62           10.93   \n",
              "2      2021-01-27 00:10:00           11.00           11.63           10.93   \n",
              "3      2021-01-27 00:15:00           11.00           11.63           10.93   \n",
              "4      2021-01-27 00:20:00           10.97           11.68           11.02   \n",
              "...                    ...             ...             ...             ...   \n",
              "77743  2021-10-31 23:35:00           10.93           11.63           10.82   \n",
              "77744  2021-10-31 23:40:00           10.92           11.63           10.83   \n",
              "77745  2021-10-31 23:45:00           10.88           11.62           10.93   \n",
              "77746  2021-10-31 23:50:00           10.88           11.60           10.83   \n",
              "77747  2021-10-31 23:55:00           10.88           11.60           10.78   \n",
              "\n",
              "       Mainline Agg_y  Mainline Agg_x  Mainline Agg_y  Mainline Agg_x  \\\n",
              "0                4.07            4.03            1.25            1.27   \n",
              "1                4.15            4.05            1.27            1.27   \n",
              "2                4.20            4.20            1.28            1.27   \n",
              "3                4.17            4.13            1.28            1.27   \n",
              "4                4.15            4.13            1.27            1.27   \n",
              "...               ...             ...             ...             ...   \n",
              "77743            4.12            4.08            1.27            1.30   \n",
              "77744            4.13            4.08            1.25            1.32   \n",
              "77745            4.13            4.08            1.28            1.30   \n",
              "77746            4.13            4.07            1.27            1.30   \n",
              "77747            4.12            4.08            1.25            1.32   \n",
              "\n",
              "       Mainline Agg_y  Mainline Agg_x  ...  Mainline Agg_y  Mainline Agg_x  \\\n",
              "0               13.15           13.13  ...           13.97           13.83   \n",
              "1               13.20           13.12  ...           13.97           13.85   \n",
              "2               13.35           13.12  ...           13.97           13.87   \n",
              "3               13.35           13.10  ...           13.95           13.87   \n",
              "4               13.33           13.12  ...           13.95           13.85   \n",
              "...               ...             ...  ...             ...             ...   \n",
              "77743           13.12           15.40  ...           14.00           13.97   \n",
              "77744           13.08           15.50  ...           14.00           13.97   \n",
              "77745           13.05           15.25  ...           14.00           13.95   \n",
              "77746           13.03           14.87  ...           13.98           13.97   \n",
              "77747           12.98           14.45  ...           13.95           13.97   \n",
              "\n",
              "       Mainline Agg_y  Mainline Agg_x  Mainline Agg_y  Mainline Agg_x  \\\n",
              "0               18.43           18.73           25.93           25.72   \n",
              "1               18.47           18.75           26.15           25.68   \n",
              "2               18.47           18.75           26.18           25.73   \n",
              "3               18.47           18.73           26.02           25.87   \n",
              "4               18.45           18.72           26.03           25.80   \n",
              "...               ...             ...             ...             ...   \n",
              "77743           18.42           18.78           25.77           25.60   \n",
              "77744           18.40           18.82           25.77           25.82   \n",
              "77745           18.40           18.82           25.82           25.73   \n",
              "77746           18.40           18.83           25.80           25.60   \n",
              "77747           18.42           18.82           25.70           25.53   \n",
              "\n",
              "       Mainline Agg_y  Mainline Agg_x  Mainline Agg_y  Mainline Agg_x  \n",
              "0                5.73            5.85           15.90           15.77  \n",
              "1                5.78            5.85           15.97           15.83  \n",
              "2                5.80            5.83           16.00           15.88  \n",
              "3                5.85            5.85           16.02           15.98  \n",
              "4                5.82            6.20           16.12           15.93  \n",
              "...               ...             ...             ...             ...  \n",
              "77743            5.75            5.92           15.73           15.82  \n",
              "77744            5.73            5.92           15.73           15.90  \n",
              "77745            5.73            5.90           15.75           15.85  \n",
              "77746            5.73            6.03           15.73           15.75  \n",
              "77747            5.73            5.92           15.70           15.72  \n",
              "\n",
              "[77748 rows x 41 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a09b6775-ba9b-4553-9ea6-6033d0cb75a0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>5 Minutes</th>\n",
              "      <th>Mainline Agg_x</th>\n",
              "      <th>Mainline Agg_y</th>\n",
              "      <th>Mainline Agg_y</th>\n",
              "      <th>Mainline Agg_y</th>\n",
              "      <th>Mainline Agg_x</th>\n",
              "      <th>Mainline Agg_y</th>\n",
              "      <th>Mainline Agg_x</th>\n",
              "      <th>Mainline Agg_y</th>\n",
              "      <th>Mainline Agg_x</th>\n",
              "      <th>...</th>\n",
              "      <th>Mainline Agg_y</th>\n",
              "      <th>Mainline Agg_x</th>\n",
              "      <th>Mainline Agg_y</th>\n",
              "      <th>Mainline Agg_x</th>\n",
              "      <th>Mainline Agg_y</th>\n",
              "      <th>Mainline Agg_x</th>\n",
              "      <th>Mainline Agg_y</th>\n",
              "      <th>Mainline Agg_x</th>\n",
              "      <th>Mainline Agg_y</th>\n",
              "      <th>Mainline Agg_x</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-01-27 00:00:00</td>\n",
              "      <td>10.95</td>\n",
              "      <td>11.57</td>\n",
              "      <td>10.92</td>\n",
              "      <td>4.07</td>\n",
              "      <td>4.03</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.27</td>\n",
              "      <td>13.15</td>\n",
              "      <td>13.13</td>\n",
              "      <td>...</td>\n",
              "      <td>13.97</td>\n",
              "      <td>13.83</td>\n",
              "      <td>18.43</td>\n",
              "      <td>18.73</td>\n",
              "      <td>25.93</td>\n",
              "      <td>25.72</td>\n",
              "      <td>5.73</td>\n",
              "      <td>5.85</td>\n",
              "      <td>15.90</td>\n",
              "      <td>15.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-01-27 00:05:00</td>\n",
              "      <td>11.00</td>\n",
              "      <td>11.62</td>\n",
              "      <td>10.93</td>\n",
              "      <td>4.15</td>\n",
              "      <td>4.05</td>\n",
              "      <td>1.27</td>\n",
              "      <td>1.27</td>\n",
              "      <td>13.20</td>\n",
              "      <td>13.12</td>\n",
              "      <td>...</td>\n",
              "      <td>13.97</td>\n",
              "      <td>13.85</td>\n",
              "      <td>18.47</td>\n",
              "      <td>18.75</td>\n",
              "      <td>26.15</td>\n",
              "      <td>25.68</td>\n",
              "      <td>5.78</td>\n",
              "      <td>5.85</td>\n",
              "      <td>15.97</td>\n",
              "      <td>15.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-01-27 00:10:00</td>\n",
              "      <td>11.00</td>\n",
              "      <td>11.63</td>\n",
              "      <td>10.93</td>\n",
              "      <td>4.20</td>\n",
              "      <td>4.20</td>\n",
              "      <td>1.28</td>\n",
              "      <td>1.27</td>\n",
              "      <td>13.35</td>\n",
              "      <td>13.12</td>\n",
              "      <td>...</td>\n",
              "      <td>13.97</td>\n",
              "      <td>13.87</td>\n",
              "      <td>18.47</td>\n",
              "      <td>18.75</td>\n",
              "      <td>26.18</td>\n",
              "      <td>25.73</td>\n",
              "      <td>5.80</td>\n",
              "      <td>5.83</td>\n",
              "      <td>16.00</td>\n",
              "      <td>15.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-01-27 00:15:00</td>\n",
              "      <td>11.00</td>\n",
              "      <td>11.63</td>\n",
              "      <td>10.93</td>\n",
              "      <td>4.17</td>\n",
              "      <td>4.13</td>\n",
              "      <td>1.28</td>\n",
              "      <td>1.27</td>\n",
              "      <td>13.35</td>\n",
              "      <td>13.10</td>\n",
              "      <td>...</td>\n",
              "      <td>13.95</td>\n",
              "      <td>13.87</td>\n",
              "      <td>18.47</td>\n",
              "      <td>18.73</td>\n",
              "      <td>26.02</td>\n",
              "      <td>25.87</td>\n",
              "      <td>5.85</td>\n",
              "      <td>5.85</td>\n",
              "      <td>16.02</td>\n",
              "      <td>15.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-01-27 00:20:00</td>\n",
              "      <td>10.97</td>\n",
              "      <td>11.68</td>\n",
              "      <td>11.02</td>\n",
              "      <td>4.15</td>\n",
              "      <td>4.13</td>\n",
              "      <td>1.27</td>\n",
              "      <td>1.27</td>\n",
              "      <td>13.33</td>\n",
              "      <td>13.12</td>\n",
              "      <td>...</td>\n",
              "      <td>13.95</td>\n",
              "      <td>13.85</td>\n",
              "      <td>18.45</td>\n",
              "      <td>18.72</td>\n",
              "      <td>26.03</td>\n",
              "      <td>25.80</td>\n",
              "      <td>5.82</td>\n",
              "      <td>6.20</td>\n",
              "      <td>16.12</td>\n",
              "      <td>15.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77743</th>\n",
              "      <td>2021-10-31 23:35:00</td>\n",
              "      <td>10.93</td>\n",
              "      <td>11.63</td>\n",
              "      <td>10.82</td>\n",
              "      <td>4.12</td>\n",
              "      <td>4.08</td>\n",
              "      <td>1.27</td>\n",
              "      <td>1.30</td>\n",
              "      <td>13.12</td>\n",
              "      <td>15.40</td>\n",
              "      <td>...</td>\n",
              "      <td>14.00</td>\n",
              "      <td>13.97</td>\n",
              "      <td>18.42</td>\n",
              "      <td>18.78</td>\n",
              "      <td>25.77</td>\n",
              "      <td>25.60</td>\n",
              "      <td>5.75</td>\n",
              "      <td>5.92</td>\n",
              "      <td>15.73</td>\n",
              "      <td>15.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77744</th>\n",
              "      <td>2021-10-31 23:40:00</td>\n",
              "      <td>10.92</td>\n",
              "      <td>11.63</td>\n",
              "      <td>10.83</td>\n",
              "      <td>4.13</td>\n",
              "      <td>4.08</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.32</td>\n",
              "      <td>13.08</td>\n",
              "      <td>15.50</td>\n",
              "      <td>...</td>\n",
              "      <td>14.00</td>\n",
              "      <td>13.97</td>\n",
              "      <td>18.40</td>\n",
              "      <td>18.82</td>\n",
              "      <td>25.77</td>\n",
              "      <td>25.82</td>\n",
              "      <td>5.73</td>\n",
              "      <td>5.92</td>\n",
              "      <td>15.73</td>\n",
              "      <td>15.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77745</th>\n",
              "      <td>2021-10-31 23:45:00</td>\n",
              "      <td>10.88</td>\n",
              "      <td>11.62</td>\n",
              "      <td>10.93</td>\n",
              "      <td>4.13</td>\n",
              "      <td>4.08</td>\n",
              "      <td>1.28</td>\n",
              "      <td>1.30</td>\n",
              "      <td>13.05</td>\n",
              "      <td>15.25</td>\n",
              "      <td>...</td>\n",
              "      <td>14.00</td>\n",
              "      <td>13.95</td>\n",
              "      <td>18.40</td>\n",
              "      <td>18.82</td>\n",
              "      <td>25.82</td>\n",
              "      <td>25.73</td>\n",
              "      <td>5.73</td>\n",
              "      <td>5.90</td>\n",
              "      <td>15.75</td>\n",
              "      <td>15.85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77746</th>\n",
              "      <td>2021-10-31 23:50:00</td>\n",
              "      <td>10.88</td>\n",
              "      <td>11.60</td>\n",
              "      <td>10.83</td>\n",
              "      <td>4.13</td>\n",
              "      <td>4.07</td>\n",
              "      <td>1.27</td>\n",
              "      <td>1.30</td>\n",
              "      <td>13.03</td>\n",
              "      <td>14.87</td>\n",
              "      <td>...</td>\n",
              "      <td>13.98</td>\n",
              "      <td>13.97</td>\n",
              "      <td>18.40</td>\n",
              "      <td>18.83</td>\n",
              "      <td>25.80</td>\n",
              "      <td>25.60</td>\n",
              "      <td>5.73</td>\n",
              "      <td>6.03</td>\n",
              "      <td>15.73</td>\n",
              "      <td>15.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77747</th>\n",
              "      <td>2021-10-31 23:55:00</td>\n",
              "      <td>10.88</td>\n",
              "      <td>11.60</td>\n",
              "      <td>10.78</td>\n",
              "      <td>4.12</td>\n",
              "      <td>4.08</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.32</td>\n",
              "      <td>12.98</td>\n",
              "      <td>14.45</td>\n",
              "      <td>...</td>\n",
              "      <td>13.95</td>\n",
              "      <td>13.97</td>\n",
              "      <td>18.42</td>\n",
              "      <td>18.82</td>\n",
              "      <td>25.70</td>\n",
              "      <td>25.53</td>\n",
              "      <td>5.73</td>\n",
              "      <td>5.92</td>\n",
              "      <td>15.70</td>\n",
              "      <td>15.72</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>77748 rows × 41 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a09b6775-ba9b-4553-9ea6-6033d0cb75a0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a09b6775-ba9b-4553-9ea6-6033d0cb75a0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a09b6775-ba9b-4553-9ea6-6033d0cb75a0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "all_corridors_tt_2021 = all_corridors_tt_2021.dropna(axis=1)\n",
        "all_corridors_tt_2021"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I33JJDadfQJl"
      },
      "outputs": [],
      "source": [
        "new_columns = [f\"travel_time_{n}\" for n in range(len(all_corridors_tt_2021.columns) - 1)]\n",
        "new_columns.insert(0, \"timestamp\")\n",
        "# len(new_columns)\n",
        "all_corridors_tt_2021 = all_corridors_tt_2021.set_axis(new_columns, axis=1, inplace=False)\n",
        "num_records, num_corridors = all_corridors_tt_2021.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTicaNTmfSWO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 804
        },
        "outputId": "c0185f34-1c40-49dc-9542-0068bc11cffe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             timestamp  travel_time_0  travel_time_1  travel_time_2  \\\n",
              "0  2021-01-27 00:00:00          10.95          11.57          10.92   \n",
              "1  2021-01-27 00:05:00          11.00          11.62          10.93   \n",
              "2  2021-01-27 00:10:00          11.00          11.63          10.93   \n",
              "3  2021-01-27 00:15:00          11.00          11.63          10.93   \n",
              "4  2021-01-27 00:20:00          10.97          11.68          11.02   \n",
              "5  2021-01-27 00:25:00          11.00          11.70          11.15   \n",
              "6  2021-01-27 00:30:00          11.02          11.68          11.17   \n",
              "7  2021-01-27 00:35:00          10.98          11.68          11.37   \n",
              "8  2021-01-27 00:40:00          10.93          11.68          11.18   \n",
              "9  2021-01-27 00:45:00          10.95          11.70          11.02   \n",
              "\n",
              "   travel_time_3  travel_time_4  travel_time_5  travel_time_6  travel_time_7  \\\n",
              "0           4.07           4.03           1.25           1.27          13.15   \n",
              "1           4.15           4.05           1.27           1.27          13.20   \n",
              "2           4.20           4.20           1.28           1.27          13.35   \n",
              "3           4.17           4.13           1.28           1.27          13.35   \n",
              "4           4.15           4.13           1.27           1.27          13.33   \n",
              "5           4.13           4.28           1.27           1.27          13.35   \n",
              "6           4.13           4.17           1.27           1.27          13.40   \n",
              "7           4.30           4.13           1.27           1.27          13.35   \n",
              "8           4.23           4.20           1.27           1.27          13.28   \n",
              "9           4.17           4.15           1.27           1.27          13.35   \n",
              "\n",
              "   travel_time_8  ...  travel_time_30  travel_time_31  travel_time_32  \\\n",
              "0          13.13  ...           13.97           13.83           18.43   \n",
              "1          13.12  ...           13.97           13.85           18.47   \n",
              "2          13.12  ...           13.97           13.87           18.47   \n",
              "3          13.10  ...           13.95           13.87           18.47   \n",
              "4          13.12  ...           13.95           13.85           18.45   \n",
              "5          13.17  ...           13.97           13.85           18.45   \n",
              "6          13.15  ...           13.97           13.85           18.45   \n",
              "7          13.12  ...           13.97           13.85           18.45   \n",
              "8          13.10  ...           14.00           13.85           18.45   \n",
              "9          13.13  ...           13.98           13.85           18.45   \n",
              "\n",
              "   travel_time_33  travel_time_34  travel_time_35  travel_time_36  \\\n",
              "0           18.73           25.93           25.72            5.73   \n",
              "1           18.75           26.15           25.68            5.78   \n",
              "2           18.75           26.18           25.73            5.80   \n",
              "3           18.73           26.02           25.87            5.85   \n",
              "4           18.72           26.03           25.80            5.82   \n",
              "5           18.72           26.15           25.97            5.80   \n",
              "6           18.72           26.03           25.97            5.78   \n",
              "7           18.72           26.00           25.85            5.80   \n",
              "8           18.73           25.95           25.77            5.78   \n",
              "9           18.73           25.83           25.78            5.80   \n",
              "\n",
              "   travel_time_37  travel_time_38  travel_time_39  \n",
              "0            5.85           15.90           15.77  \n",
              "1            5.85           15.97           15.83  \n",
              "2            5.83           16.00           15.88  \n",
              "3            5.85           16.02           15.98  \n",
              "4            6.20           16.12           15.93  \n",
              "5            5.98           16.13           16.02  \n",
              "6            5.92           16.02           15.98  \n",
              "7            5.87           15.97           15.95  \n",
              "8            6.32           15.97           15.90  \n",
              "9            6.75           15.93           15.92  \n",
              "\n",
              "[10 rows x 41 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b1c74df1-f0f1-4c99-b233-78aea26f1478\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>travel_time_0</th>\n",
              "      <th>travel_time_1</th>\n",
              "      <th>travel_time_2</th>\n",
              "      <th>travel_time_3</th>\n",
              "      <th>travel_time_4</th>\n",
              "      <th>travel_time_5</th>\n",
              "      <th>travel_time_6</th>\n",
              "      <th>travel_time_7</th>\n",
              "      <th>travel_time_8</th>\n",
              "      <th>...</th>\n",
              "      <th>travel_time_30</th>\n",
              "      <th>travel_time_31</th>\n",
              "      <th>travel_time_32</th>\n",
              "      <th>travel_time_33</th>\n",
              "      <th>travel_time_34</th>\n",
              "      <th>travel_time_35</th>\n",
              "      <th>travel_time_36</th>\n",
              "      <th>travel_time_37</th>\n",
              "      <th>travel_time_38</th>\n",
              "      <th>travel_time_39</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-01-27 00:00:00</td>\n",
              "      <td>10.95</td>\n",
              "      <td>11.57</td>\n",
              "      <td>10.92</td>\n",
              "      <td>4.07</td>\n",
              "      <td>4.03</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.27</td>\n",
              "      <td>13.15</td>\n",
              "      <td>13.13</td>\n",
              "      <td>...</td>\n",
              "      <td>13.97</td>\n",
              "      <td>13.83</td>\n",
              "      <td>18.43</td>\n",
              "      <td>18.73</td>\n",
              "      <td>25.93</td>\n",
              "      <td>25.72</td>\n",
              "      <td>5.73</td>\n",
              "      <td>5.85</td>\n",
              "      <td>15.90</td>\n",
              "      <td>15.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-01-27 00:05:00</td>\n",
              "      <td>11.00</td>\n",
              "      <td>11.62</td>\n",
              "      <td>10.93</td>\n",
              "      <td>4.15</td>\n",
              "      <td>4.05</td>\n",
              "      <td>1.27</td>\n",
              "      <td>1.27</td>\n",
              "      <td>13.20</td>\n",
              "      <td>13.12</td>\n",
              "      <td>...</td>\n",
              "      <td>13.97</td>\n",
              "      <td>13.85</td>\n",
              "      <td>18.47</td>\n",
              "      <td>18.75</td>\n",
              "      <td>26.15</td>\n",
              "      <td>25.68</td>\n",
              "      <td>5.78</td>\n",
              "      <td>5.85</td>\n",
              "      <td>15.97</td>\n",
              "      <td>15.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-01-27 00:10:00</td>\n",
              "      <td>11.00</td>\n",
              "      <td>11.63</td>\n",
              "      <td>10.93</td>\n",
              "      <td>4.20</td>\n",
              "      <td>4.20</td>\n",
              "      <td>1.28</td>\n",
              "      <td>1.27</td>\n",
              "      <td>13.35</td>\n",
              "      <td>13.12</td>\n",
              "      <td>...</td>\n",
              "      <td>13.97</td>\n",
              "      <td>13.87</td>\n",
              "      <td>18.47</td>\n",
              "      <td>18.75</td>\n",
              "      <td>26.18</td>\n",
              "      <td>25.73</td>\n",
              "      <td>5.80</td>\n",
              "      <td>5.83</td>\n",
              "      <td>16.00</td>\n",
              "      <td>15.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-01-27 00:15:00</td>\n",
              "      <td>11.00</td>\n",
              "      <td>11.63</td>\n",
              "      <td>10.93</td>\n",
              "      <td>4.17</td>\n",
              "      <td>4.13</td>\n",
              "      <td>1.28</td>\n",
              "      <td>1.27</td>\n",
              "      <td>13.35</td>\n",
              "      <td>13.10</td>\n",
              "      <td>...</td>\n",
              "      <td>13.95</td>\n",
              "      <td>13.87</td>\n",
              "      <td>18.47</td>\n",
              "      <td>18.73</td>\n",
              "      <td>26.02</td>\n",
              "      <td>25.87</td>\n",
              "      <td>5.85</td>\n",
              "      <td>5.85</td>\n",
              "      <td>16.02</td>\n",
              "      <td>15.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-01-27 00:20:00</td>\n",
              "      <td>10.97</td>\n",
              "      <td>11.68</td>\n",
              "      <td>11.02</td>\n",
              "      <td>4.15</td>\n",
              "      <td>4.13</td>\n",
              "      <td>1.27</td>\n",
              "      <td>1.27</td>\n",
              "      <td>13.33</td>\n",
              "      <td>13.12</td>\n",
              "      <td>...</td>\n",
              "      <td>13.95</td>\n",
              "      <td>13.85</td>\n",
              "      <td>18.45</td>\n",
              "      <td>18.72</td>\n",
              "      <td>26.03</td>\n",
              "      <td>25.80</td>\n",
              "      <td>5.82</td>\n",
              "      <td>6.20</td>\n",
              "      <td>16.12</td>\n",
              "      <td>15.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2021-01-27 00:25:00</td>\n",
              "      <td>11.00</td>\n",
              "      <td>11.70</td>\n",
              "      <td>11.15</td>\n",
              "      <td>4.13</td>\n",
              "      <td>4.28</td>\n",
              "      <td>1.27</td>\n",
              "      <td>1.27</td>\n",
              "      <td>13.35</td>\n",
              "      <td>13.17</td>\n",
              "      <td>...</td>\n",
              "      <td>13.97</td>\n",
              "      <td>13.85</td>\n",
              "      <td>18.45</td>\n",
              "      <td>18.72</td>\n",
              "      <td>26.15</td>\n",
              "      <td>25.97</td>\n",
              "      <td>5.80</td>\n",
              "      <td>5.98</td>\n",
              "      <td>16.13</td>\n",
              "      <td>16.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2021-01-27 00:30:00</td>\n",
              "      <td>11.02</td>\n",
              "      <td>11.68</td>\n",
              "      <td>11.17</td>\n",
              "      <td>4.13</td>\n",
              "      <td>4.17</td>\n",
              "      <td>1.27</td>\n",
              "      <td>1.27</td>\n",
              "      <td>13.40</td>\n",
              "      <td>13.15</td>\n",
              "      <td>...</td>\n",
              "      <td>13.97</td>\n",
              "      <td>13.85</td>\n",
              "      <td>18.45</td>\n",
              "      <td>18.72</td>\n",
              "      <td>26.03</td>\n",
              "      <td>25.97</td>\n",
              "      <td>5.78</td>\n",
              "      <td>5.92</td>\n",
              "      <td>16.02</td>\n",
              "      <td>15.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2021-01-27 00:35:00</td>\n",
              "      <td>10.98</td>\n",
              "      <td>11.68</td>\n",
              "      <td>11.37</td>\n",
              "      <td>4.30</td>\n",
              "      <td>4.13</td>\n",
              "      <td>1.27</td>\n",
              "      <td>1.27</td>\n",
              "      <td>13.35</td>\n",
              "      <td>13.12</td>\n",
              "      <td>...</td>\n",
              "      <td>13.97</td>\n",
              "      <td>13.85</td>\n",
              "      <td>18.45</td>\n",
              "      <td>18.72</td>\n",
              "      <td>26.00</td>\n",
              "      <td>25.85</td>\n",
              "      <td>5.80</td>\n",
              "      <td>5.87</td>\n",
              "      <td>15.97</td>\n",
              "      <td>15.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2021-01-27 00:40:00</td>\n",
              "      <td>10.93</td>\n",
              "      <td>11.68</td>\n",
              "      <td>11.18</td>\n",
              "      <td>4.23</td>\n",
              "      <td>4.20</td>\n",
              "      <td>1.27</td>\n",
              "      <td>1.27</td>\n",
              "      <td>13.28</td>\n",
              "      <td>13.10</td>\n",
              "      <td>...</td>\n",
              "      <td>14.00</td>\n",
              "      <td>13.85</td>\n",
              "      <td>18.45</td>\n",
              "      <td>18.73</td>\n",
              "      <td>25.95</td>\n",
              "      <td>25.77</td>\n",
              "      <td>5.78</td>\n",
              "      <td>6.32</td>\n",
              "      <td>15.97</td>\n",
              "      <td>15.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2021-01-27 00:45:00</td>\n",
              "      <td>10.95</td>\n",
              "      <td>11.70</td>\n",
              "      <td>11.02</td>\n",
              "      <td>4.17</td>\n",
              "      <td>4.15</td>\n",
              "      <td>1.27</td>\n",
              "      <td>1.27</td>\n",
              "      <td>13.35</td>\n",
              "      <td>13.13</td>\n",
              "      <td>...</td>\n",
              "      <td>13.98</td>\n",
              "      <td>13.85</td>\n",
              "      <td>18.45</td>\n",
              "      <td>18.73</td>\n",
              "      <td>25.83</td>\n",
              "      <td>25.78</td>\n",
              "      <td>5.80</td>\n",
              "      <td>6.75</td>\n",
              "      <td>15.93</td>\n",
              "      <td>15.92</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 41 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b1c74df1-f0f1-4c99-b233-78aea26f1478')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b1c74df1-f0f1-4c99-b233-78aea26f1478 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b1c74df1-f0f1-4c99-b233-78aea26f1478');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "all_corridors_tt_2021.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoI460TTfU-v"
      },
      "outputs": [],
      "source": [
        "def make_dataset_many_to_one(array,time_steps, num_sample, Horizon = 0):\n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    indices = random.sample(range(time_steps, len(array) - Horizon), num_sample)\n",
        "\n",
        "    for i in indices:\n",
        "        x.append(array[i-time_steps:i])\n",
        "        y.append(array[i + Horizon])\n",
        "        \n",
        "    return np.array(x), np.array(y)\n",
        "\n",
        "# train_data = all_corridors_tt_2021.drop(['timestamp'], axis=1).iloc[:num_training]\n",
        "total_data = all_corridors_tt_2021.drop(['timestamp'], axis=1)\n",
        " #for testing at bottom\n",
        "# print(total_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LWVvhxkfX0l"
      },
      "outputs": [],
      "source": [
        "#scale values to [0,1]\n",
        "num_training = 12000\n",
        "num_validate = 4000\n",
        "num_test = 4000\n",
        "time_steps = 24  # 2 hours\n",
        "num_corridor = total_data.shape[1]\n",
        "total = total_data.to_numpy()\n",
        "# scaler = MinMaxScaler(feature_range=(0,1))\n",
        "# total = scaler.fit_transform(total_data)\n",
        "# # train = scaler.transform(train_data)\n",
        "# total = scaler.transform(total_data)\n",
        "\n",
        "# sc = MinMaxScaler()\n",
        "# total_data = sc.fit_transform(total_data)\n",
        "\n",
        "## 15 minutes horizon\n",
        "trn_x_15min, trn_y_15min = make_dataset_many_to_one(total, time_steps, num_training, Horizon = 2)  #every 2 hours.\n",
        "vld_x_15min, vld_y_15min = make_dataset_many_to_one(total, time_steps, num_validate, Horizon = 2)  #every 2 hours.\n",
        "tst_x_15min, tst_y_15min = make_dataset_many_to_one(total, time_steps, num_test, Horizon = 2)  #every 2 hours.\n",
        "\n",
        "trn_x_15min = trn_x_15min.reshape(-1,time_steps, num_corridor)\n",
        "vld_x_15min = vld_x_15min.reshape(-1,time_steps, num_corridor)\n",
        "tst_x_15min = tst_x_15min.reshape(-1,time_steps, num_corridor)\n",
        "\n",
        "\n",
        "## 30 minutes horizon\n",
        "trn_x_30min, trn_y_30min = make_dataset_many_to_one(total, time_steps, num_training, Horizon = 5)  #every 2 hours.\n",
        "vld_x_30min, vld_y_30min = make_dataset_many_to_one(total, time_steps, num_validate, Horizon = 5)  #every 2 hours.\n",
        "tst_x_30min, tst_y_30min = make_dataset_many_to_one(total, time_steps, num_test, Horizon = 5)  #every 2 hours.\n",
        "\n",
        "trn_x_30min = trn_x_30min.reshape(-1,time_steps, num_corridor)\n",
        "vld_x_30min = vld_x_30min.reshape(-1,time_steps, num_corridor)\n",
        "tst_x_30min = tst_x_30min.reshape(-1,time_steps, num_corridor)\n",
        "\n",
        "\n",
        "## 45 minutes horizon\n",
        "trn_x_45min, trn_y_45min = make_dataset_many_to_one(total, time_steps, num_training, Horizon = 8)  #every 2 hours.\n",
        "vld_x_45min, vld_y_45min = make_dataset_many_to_one(total, time_steps, num_validate, Horizon = 8)  #every 2 hours.\n",
        "tst_x_45min, tst_y_45min = make_dataset_many_to_one(total, time_steps, num_test, Horizon = 8)  #every 2 hours.\n",
        "\n",
        "trn_x_45min = trn_x_45min.reshape(-1,time_steps, num_corridor)\n",
        "vld_x_45min = vld_x_45min.reshape(-1,time_steps, num_corridor)\n",
        "tst_x_45min = tst_x_45min.reshape(-1,time_steps, num_corridor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tArn7L3_0wZ"
      },
      "outputs": [],
      "source": [
        "# conver the data to tensors and pass to the Dataloader \n",
        "# to create an batch iterator\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class MyData(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.data = X\n",
        "        self.target = y\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.target[index]\n",
        "        return x, y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSMU364amW6u"
      },
      "outputs": [],
      "source": [
        "class SelfAttentionPooling(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(SelfAttentionPooling, self).__init__()\n",
        "        self.W = nn.Linear(input_dim, 1)\n",
        "    \n",
        "    def forward(self, batch_rep):\n",
        "        \"\"\"\n",
        "        input:\n",
        "            batch_rep : size (N, T, H), N: batch size, T: sequence length, H: Hidden dimension\n",
        "      \n",
        "        attention_weight:\n",
        "            att_w : size (N, T, 1)\n",
        "    \n",
        "        return:\n",
        "            utter_rep: size (N, H)\n",
        "        \"\"\"\n",
        "        \n",
        "        softmax = nn.functional.softmax\n",
        "        att_w = softmax(self.W(batch_rep).squeeze(-1)).unsqueeze(-1)\n",
        "        utter_rep = torch.sum(batch_rep * att_w, dim=1)\n",
        "\n",
        "        return utter_rep\n",
        "\n",
        "\n",
        "# A custom attention layer\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, attention_size, att_hops, non_linearity=\"tanh\"):\n",
        "        super(SelfAttention, self).__init__()\n",
        "\n",
        "        self.ut_dense =  nn.Sequential(\n",
        "                nn.Linear(hidden_size, attention_size),\n",
        "                nn.Tanh()\n",
        "         )\n",
        "        \n",
        "        self.et_dense = nn.Linear(attention_size, att_hops)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        ##################################################################\n",
        "        # STEP 1 - perform dot product\n",
        "        # of the attention vector and each hidden state\n",
        "        ##################################################################\n",
        "\n",
        "        # inputs is a 3D Tensor: batch, len, hidden_size\n",
        "        # scores is a 2D Tensor: batch, len\n",
        "        ut = self.ut_dense(inputs)\n",
        "\n",
        "        # et shape: [batch_size, seq_len, att_hops]\n",
        "        et = self.et_dense(ut)\n",
        "\n",
        "        att_scores = self.softmax(torch.permute(et, (0, 2, 1)))\n",
        "\n",
        "        # # re-normalize the masked scores\n",
        "        # _sums = scores.sum(-1, keepdim=True)  # sums per row\n",
        "        # att_scores = scores.div(_sums)  # divide by row sum\n",
        "\n",
        "        ##################################################################\n",
        "        # Step 2 - Weighted sum of hidden states, by the attention scores\n",
        "        ##################################################################\n",
        "        \n",
        "        # print(\"att_scores.shape: \", att_scores.shape, \"inputs.shape\", inputs.shape)\n",
        "\n",
        "        # multiply each hidden state with the attention weights\n",
        "        output = torch.bmm(att_scores, inputs)\n",
        "\n",
        "        return output, att_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C96RbNO3MYTk"
      },
      "outputs": [],
      "source": [
        "class HierLstmat(nn.Module):\n",
        "\n",
        "    def __init__(self, num_corridor, hidden_size, num_layers, natt_unit, natt_hops, nfc):\n",
        "        super(HierLstmat, self).__init__()\n",
        "\n",
        "        self.input_size = num_corridor\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.sequence_len = 0\n",
        "    \n",
        "        # Initialize LSTM Cell for the first layer\n",
        "        self.lstm_cell_layer_1 = nn.LSTMCell(self.input_size, self.hidden_size)\n",
        "\n",
        "        # Initialize LSTM Cell for the second layer\n",
        "        self.lstm_cell_layer_2 = nn.LSTMCell(self.hidden_size, self.hidden_size)\n",
        "\n",
        "        # default maximum upgrade length\n",
        "        self.up_len = 80      \n",
        "\n",
        "        # flattern\n",
        "        self.handle_hops = nn.Sequential(\n",
        "            nn.Flatten()\n",
        "            )\n",
        "\n",
        "        # output layer\n",
        "        self.output_layer =  nn.Sequential(\n",
        "            nn.Linear(self.hidden_size * natt_hops, nfc),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(nfc, num_corridor)\n",
        "        )\n",
        "\n",
        "        # attention layer\n",
        "        self.att_encoder = SelfAttention(natt_unit, natt_hops)      \n",
        "        \n",
        "        # attention pooling layer\n",
        "        self.att_pooling = SelfAttentionPooling(self.hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "      self.sequence_len = x.shape[1]\n",
        "      batch_Size = x.shape[0]\n",
        "      sequence_input = x.transpose(0, 1)\n",
        "\n",
        "      # print(\"sequence_input.device: \", sequence_input.device)\n",
        "      \n",
        "      # batch_size x hidden_size\n",
        "      hidden_state = torch.zeros(batch_Size, self.hidden_size).to(device)\n",
        "      cell_state = torch.zeros(batch_Size, self.hidden_size).to(device)\n",
        "      hidden_state_2 = torch.zeros(batch_Size, self.hidden_size).to(device)\n",
        "      cell_state_2 = torch.zeros(batch_Size, self.hidden_size).to(device)\n",
        "\n",
        "      # print(\"hidden_state.device: \", hidden_state.device)\n",
        "      \n",
        "      # weights initialization\n",
        "      torch.nn.init.xavier_normal_(hidden_state)\n",
        "      torch.nn.init.xavier_normal_(cell_state)\n",
        "      torch.nn.init.xavier_normal_(hidden_state_2)\n",
        "      torch.nn.init.xavier_normal_(cell_state_2)\n",
        "\n",
        "      # set upgrade length\n",
        "      up_len = min(self.up_len, math.floor(math.sqrt(self.sequence_len)))\n",
        "      # evenly spaced index\n",
        "      idx = np.linspace(up_len - 1, math.pow(up_len, 2) - 1, num = up_len)\n",
        "      # print(\"sequence index: \", idx)\n",
        "\n",
        "      # initiate pooling hidden_sates an cell states\n",
        "      interverl_hidden_states = torch.empty(batch_Size, self.hidden_size, device = device)\n",
        "      interverl_cell_states = torch.empty(batch_Size, self.hidden_size, device = device)\n",
        "      outer_hidden_states = torch.empty(batch_Size, self.hidden_size, device = device)\n",
        "      outer_cell_states = torch.empty(batch_Size, self.hidden_size, device = device)\n",
        "      \n",
        "      # Unfolding LSTM\n",
        "      # Last hidden_state will be used to feed the fully connected neural net\n",
        "      for i in range(self.sequence_len):\n",
        "        \n",
        "        hidden_state, cell_state = self.lstm_cell_layer_1(sequence_input[i], (hidden_state, cell_state))\n",
        "\n",
        "        if  torch.isnan(interverl_hidden_states).sum() == 0 :\n",
        "            interverl_hidden_states = hidden_state[None, :]\n",
        "            interverl_cell_states = cell_state[None, :]\n",
        "\n",
        "        else:\n",
        "            interverl_hidden_states = torch.cat((interverl_hidden_states, hidden_state[None, :]), 0) # TimeSteps * Batch  * Feature\n",
        "            interverl_cell_states = torch.cat((interverl_cell_states, cell_state[None, :]), 0)       # TimeSteps * Batch  * Feature\n",
        "            # print(\"interverl_hidden_states: \", interverl_hidden_states.shape)\n",
        "\n",
        "        if i in idx or (i == self.sequence_len - 1):\n",
        "\n",
        "            interverl_hidden_states = interverl_hidden_states.transpose(0,1)\n",
        "            interverl_cell_states = interverl_cell_states.transpose(0,1)\n",
        "\n",
        "            # print(\"interverl_hidden_states: \", interverl_hidden_states.shape)\n",
        "            # print(\"interverl_cell_states: \", interverl_cell_states.shape)\n",
        "\n",
        "            layer1_cell_states = torch.cat((interverl_cell_states, cell_state_2[None, :].transpose(0,1)), 1)       # Batch * (TimeSteps + 1) * Feature\n",
        "\n",
        "            layer2_input = self.att_pooling(interverl_hidden_states)   # Batch * Feature\n",
        "            cell_state_2 = self.att_pooling(layer1_cell_states)       # Batch * Feature\n",
        "            \n",
        "            hidden_state_2, cell_state_2 = self.lstm_cell_layer_2(layer2_input, (hidden_state_2, cell_state_2))\n",
        "            interverl_hidden_states = torch.empty(batch_Size, self.hidden_size, device = device)\n",
        "            interverl_cell_states = torch.empty(batch_Size, self.hidden_size, device = device)\n",
        "\n",
        "            if torch.isnan(outer_hidden_states).sum() == 0:\n",
        "              outer_hidden_states = hidden_state_2[None, :]\n",
        "            else:\n",
        "              outer_hidden_states = torch.cat((outer_hidden_states, hidden_state_2[None, :]))  # # Sequence * Batch  * hiddensize\n",
        "\n",
        "      up_x = torch.transpose(outer_hidden_states, 1, 0)   # size: (batch, Sequence, hiddensize)\n",
        "      # print(\"up_x.shape: \", up_x.shape)   # up_x.shape:  torch.Size([250, 120])\n",
        "      # print(\"reshaped up_x: \", up_x.view(batch_Size, -1, self.hidden_size).shape)   # reshaped up_x:  torch.Size([batch_size, sample_Size, hidden_size])\n",
        "\n",
        "      att_output, att_scores = self.att_encoder(up_x.view(batch_Size, -1, self.hidden_size))\n",
        "      # att_output shape [batch_size, att_hops, LSTM_nhidden]\n",
        "\n",
        "      att_output_flattern = self.handle_hops(att_output)  # [batch_size, att_hops * LSTM_nhidden]\n",
        "      # print(\"att_output_flattern: \", att_output_flattern.shape)\n",
        "\n",
        "      # Last hidden state is passed through a fully connected neural net\n",
        "      output = self.output_layer(att_output_flattern)\n",
        "      \n",
        "      return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_laNAeVZJs70"
      },
      "outputs": [],
      "source": [
        "num_corridor =  num_corridor       # number of corridors also the channel in NTC\n",
        "hidden_size = 120        # lstm hidden_dim\n",
        "num_layers = 1          # lstm layers\n",
        "attention_size = 300     # the hidden_units of attention layer\n",
        "natt_hops = 2\n",
        "nfc = 512               # fully connected layer\n",
        "drop_prob = 0.5         # fully connected layer\n",
        "batch_size = 50\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = HierLstmat(num_corridor, hidden_size, num_layers, attention_size, natt_hops, nfc)\n",
        "model = model.to(device)\n",
        "\n",
        "learning_rate =  0.001\n",
        "\n",
        "num_epochs = 1000\n",
        "\n",
        "criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "inputs = torch.randn(batch_size, 48, num_corridor).to(device)\n",
        "\n",
        "y_pred = model(inputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpG-1hFEaNTQ"
      },
      "source": [
        "## 15 minutes Horizon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_kRN5XdfhSU",
        "outputId": "62ecb720-6bfb-4df9-f03a-88aa4e2c81c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "epoch: 10 | trn_loss: 0.79607413 val_loss: 0.72543999 \n",
            "epoch: 20 | trn_loss: 0.48407087 val_loss: 0.47852132 \n",
            "epoch: 30 | trn_loss: 0.36052834 val_loss: 0.36098687 \n",
            "epoch: 40 | trn_loss: 0.29872425 val_loss: 0.30487633 \n",
            "epoch: 50 | trn_loss: 0.25987005 val_loss: 0.26286122 \n",
            "epoch: 60 | trn_loss: 0.22659169 val_loss: 0.21906827 \n",
            "epoch: 70 | trn_loss: 0.20446902 val_loss: 0.19773974 \n",
            "epoch: 80 | trn_loss: 0.19030888 val_loss: 0.18604390 \n",
            "epoch: 90 | trn_loss: 0.18081375 val_loss: 0.18500297 \n",
            "epoch: 100 | trn_loss: 0.17395390 val_loss: 0.18504540 \n",
            "epoch: 110 | trn_loss: 0.16630591 val_loss: 0.18193275 \n",
            "epoch: 120 | trn_loss: 0.15773425 val_loss: 0.17407007 \n",
            "epoch: 130 | trn_loss: 0.15050213 val_loss: 0.17957285 \n",
            "epoch: 140 | trn_loss: 0.14665295 val_loss: 0.17305557 \n",
            "epoch: 150 | trn_loss: 0.14176797 val_loss: 0.17133603 \n",
            "epoch: 160 | trn_loss: 0.13964987 val_loss: 0.16889672 \n",
            "epoch: 170 | trn_loss: 0.13211513 val_loss: 0.16774309 \n",
            "epoch: 180 | trn_loss: 0.12885278 val_loss: 0.16538640 \n",
            "epoch: 190 | trn_loss: 0.12657545 val_loss: 0.16918800 \n",
            "epoch: 200 | trn_loss: 0.12095050 val_loss: 0.17096327 \n",
            "epoch: 210 | trn_loss: 0.11816386 val_loss: 0.18303829 \n",
            "epoch: 220 | trn_loss: 0.12466865 val_loss: 0.17661273 \n",
            "epoch: 230 | trn_loss: 0.11223700 val_loss: 0.17593891 \n",
            "epoch: 240 | trn_loss: 0.10943800 val_loss: 0.18474294 \n",
            "epoch: 250 | trn_loss: 0.10834544 val_loss: 0.17368938 \n",
            "epoch: 260 | trn_loss: 0.11872731 val_loss: 0.18030505 \n",
            "epoch: 270 | trn_loss: 0.10975002 val_loss: 0.18916389 \n",
            "epoch: 280 | trn_loss: 0.10165486 val_loss: 0.17652283 \n",
            "epoch: 290 | trn_loss: 0.10668179 val_loss: 0.18581192 \n",
            "epoch: 300 | trn_loss: 0.09598505 val_loss: 0.17867207 \n",
            "epoch: 310 | trn_loss: 0.09743055 val_loss: 0.17673371 \n",
            "epoch: 320 | trn_loss: 0.09734831 val_loss: 0.16925410 \n",
            "epoch: 330 | trn_loss: 0.09270009 val_loss: 0.16640851 \n",
            "epoch: 340 | trn_loss: 0.09506151 val_loss: 0.17355845 \n",
            "epoch: 350 | trn_loss: 0.09636918 val_loss: 0.17241385 \n",
            "epoch: 360 | trn_loss: 0.09185601 val_loss: 0.17557217 \n",
            "epoch: 370 | trn_loss: 0.10133187 val_loss: 0.18880787 \n",
            "epoch: 380 | trn_loss: 0.08727021 val_loss: 0.17811940 \n",
            "epoch: 390 | trn_loss: 0.08519422 val_loss: 0.17348203 \n",
            "epoch: 400 | trn_loss: 0.08485499 val_loss: 0.16433642 \n",
            "epoch: 410 | trn_loss: 0.09291961 val_loss: 0.18506911 \n",
            "epoch: 420 | trn_loss: 0.08349841 val_loss: 0.17006139 \n",
            "epoch: 430 | trn_loss: 0.08227189 val_loss: 0.17890131 \n",
            "epoch: 440 | trn_loss: 0.09153278 val_loss: 0.17205156 \n",
            "epoch: 450 | trn_loss: 0.07807150 val_loss: 0.16251103 \n",
            "epoch: 460 | trn_loss: 0.07950325 val_loss: 0.16537598 \n",
            "epoch: 470 | trn_loss: 0.08144734 val_loss: 0.16868553 \n",
            "epoch: 480 | trn_loss: 0.08488506 val_loss: 0.16719043 \n",
            "epoch: 490 | trn_loss: 0.07891928 val_loss: 0.16430430 \n",
            "epoch: 500 | trn_loss: 0.07646945 val_loss: 0.22420298 \n",
            "epoch: 510 | trn_loss: 0.07359071 val_loss: 0.16699308 \n",
            "epoch: 520 | trn_loss: 0.07719015 val_loss: 0.16572395 \n",
            "epoch: 530 | trn_loss: 0.07762334 val_loss: 0.16598256 \n",
            "epoch: 540 | trn_loss: 0.07627497 val_loss: 0.16858964 \n",
            "epoch: 550 | trn_loss: 0.07231157 val_loss: 0.15889605 \n",
            "epoch: 560 | trn_loss: 0.07481421 val_loss: 0.16485336 \n",
            "epoch: 570 | trn_loss: 0.06854503 val_loss: 0.19156376 \n",
            "epoch: 580 | trn_loss: 0.06962834 val_loss: 0.16702940 \n",
            "epoch: 590 | trn_loss: 0.07075375 val_loss: 0.17335541 \n",
            "epoch: 600 | trn_loss: 0.06554137 val_loss: 0.17124827 \n",
            "epoch: 610 | trn_loss: 0.07167549 val_loss: 0.18305712 \n",
            "epoch: 620 | trn_loss: 0.06598555 val_loss: 0.17184625 \n",
            "epoch: 630 | trn_loss: 0.07246876 val_loss: 0.17635459 \n",
            "epoch: 640 | trn_loss: 0.07601473 val_loss: 0.18404000 \n",
            "epoch: 650 | trn_loss: 0.06580232 val_loss: 0.16954971 \n",
            "epoch: 660 | trn_loss: 0.06477953 val_loss: 0.18604082 \n",
            "epoch: 670 | trn_loss: 0.06827349 val_loss: 0.18300713 \n",
            "epoch: 680 | trn_loss: 0.06896042 val_loss: 0.17956643 \n",
            "epoch: 690 | trn_loss: 0.06572047 val_loss: 0.17528317 \n",
            "epoch: 700 | trn_loss: 0.06626846 val_loss: 0.17605655 \n",
            "epoch: 710 | trn_loss: 0.06283906 val_loss: 0.17431706 \n",
            "epoch: 720 | trn_loss: 0.06341214 val_loss: 0.17136042 \n",
            "epoch: 730 | trn_loss: 0.06384124 val_loss: 0.17461641 \n",
            "epoch: 740 | trn_loss: 0.06709427 val_loss: 0.17467250 \n",
            "epoch: 750 | trn_loss: 0.06391968 val_loss: 0.16499895 \n",
            "epoch: 760 | trn_loss: 0.06043912 val_loss: 0.16338975 \n",
            "epoch: 770 | trn_loss: 0.06249154 val_loss: 0.17453076 \n",
            "epoch: 780 | trn_loss: 0.06442788 val_loss: 0.17491305 \n",
            "epoch: 790 | trn_loss: 0.06006459 val_loss: 0.16903823 \n",
            "epoch: 800 | trn_loss: 0.06213295 val_loss: 0.18402514 \n",
            "epoch: 810 | trn_loss: 0.06788094 val_loss: 0.18401816 \n",
            "epoch: 820 | trn_loss: 0.05702983 val_loss: 0.17033325 \n",
            "epoch: 830 | trn_loss: 0.05561145 val_loss: 0.16511179 \n",
            "epoch: 840 | trn_loss: 0.06183904 val_loss: 0.17188841 \n",
            "epoch: 850 | trn_loss: 0.05677161 val_loss: 0.17424331 \n",
            "epoch: 860 | trn_loss: 0.05889911 val_loss: 0.17162034 \n",
            "epoch: 870 | trn_loss: 0.05581529 val_loss: 0.16800388 \n",
            "epoch: 880 | trn_loss: 0.05747164 val_loss: 0.16691687 \n",
            "epoch: 890 | trn_loss: 0.05888910 val_loss: 0.17630930 \n",
            "epoch: 900 | trn_loss: 0.05579699 val_loss: 0.17352668 \n",
            "epoch: 910 | trn_loss: 0.05274830 val_loss: 0.17314086 \n",
            "epoch: 920 | trn_loss: 0.05439767 val_loss: 0.17385789 \n",
            "epoch: 930 | trn_loss: 0.05581632 val_loss: 0.16879945 \n",
            "epoch: 940 | trn_loss: 0.06252851 val_loss: 0.17221071 \n",
            "epoch: 950 | trn_loss: 0.05109300 val_loss: 0.16468787 \n",
            "epoch: 960 | trn_loss: 0.05366118 val_loss: 0.17344820 \n",
            "epoch: 970 | trn_loss: 0.05369061 val_loss: 0.16941937 \n",
            "epoch: 980 | trn_loss: 0.05286847 val_loss: 0.17249405 \n",
            "epoch: 990 | trn_loss: 0.05338973 val_loss: 0.17169158 \n",
            "epoch: 1000 | trn_loss: 0.05389245 val_loss: 0.17693544 \n",
            "time 7113.67 sec\n",
            "****************************************************************************************************\n",
            "mae_loss, rmse_loss, mape_loss:  0.19515277203172446 0.3397937573492527 0.01896775346249342\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, mean_absolute_error\n",
        "\n",
        "trainX = Variable(torch.Tensor(np.array(trn_x_15min)))\n",
        "trainY = Variable(torch.Tensor(np.array(trn_y_15min)))\n",
        "\n",
        "vldtX = Variable(torch.Tensor(np.array(vld_x_15min)))\n",
        "vldtY = Variable(torch.Tensor(np.array(vld_y_15min)))\n",
        "\n",
        "testX = Variable(torch.Tensor(np.array(tst_x_15min)))\n",
        "testY = Variable(torch.Tensor(np.array(tst_y_15min)))\n",
        "\n",
        "train_dataset = MyData(trainX, trainY)\n",
        "val_dataset = MyData(vldtX, vldtY)\n",
        "tst_dataset = MyData(testX, testY)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = batch_size)\n",
        "val_loader = DataLoader(val_dataset, batch_size = batch_size)\n",
        "tst_dataset = DataLoader(tst_dataset, batch_size = batch_size)\n",
        "\n",
        "trn_loss, val_loss = [], []\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "print(device)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "\n",
        "    ls = 0\n",
        "    valid_ls = 0\n",
        "\n",
        "    for i, train_batch in enumerate(train_loader):\n",
        "        inputs, targets = train_batch\n",
        "\n",
        "        # Move tensors to the configured device\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        model.train()\n",
        "        y_pred = model(inputs)\n",
        "        loss = criterion(y_pred, targets)\n",
        "\n",
        "        ls += loss.item()\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    avg_ls = ls/(i + 1)\n",
        "    trn_loss.append(avg_ls)\n",
        "\n",
        "    # Validating the model with current parameters\n",
        "\n",
        "    for j, val_batch in enumerate(val_loader):\n",
        "        inputs, targets = val_batch\n",
        "\n",
        "        # Move tensors to the configured device\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        model.eval()\n",
        "        y_pred = model(inputs)\n",
        "        loss = criterion(y_pred, targets)\n",
        "        valid_ls += loss.item()\n",
        "\n",
        "    avg_valid_ls = valid_ls/(j + 1)\n",
        "\n",
        "    if  len(val_loss) == 0 or avg_valid_ls < min(val_loss):\n",
        "\n",
        "        filepath = f'/content/drive/MyDrive/CIS 545 Project Folder/Data Set/hiLSTMat_15min_epoch_{epoch}_loss_{avg_valid_ls}.pth'\n",
        "\n",
        "        torch.save(model.state_dict(), filepath)\n",
        "        \n",
        "        mae_test,  rmse_test,  mape_test = 0, 0, 0\n",
        "\n",
        "        for k, tst_batch in enumerate(tst_dataset):\n",
        "\n",
        "            inputs, targets = tst_batch\n",
        "\n",
        "            # Move tensors to the configured device\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            model.eval()\n",
        "            y_pred = model(inputs)\n",
        "\n",
        "            y_pred = y_pred.cpu().detach().numpy()\n",
        "            targets = targets.cpu().detach().numpy()\n",
        "\n",
        "            # y_pred = scaler.inverse_transform(y_pred)\n",
        "            # targets = scaler.inverse_transform(targets)   \n",
        "\n",
        "            mae_test += mean_absolute_error(targets, y_pred)\n",
        "            \n",
        "            rmse_test += mean_squared_error(targets, y_pred, squared=False)\n",
        "\n",
        "            mape_test += mean_absolute_percentage_error(targets, y_pred)\n",
        "\n",
        "        mae_loss, rmse_loss, mape_loss = mae_test/(k+1), rmse_test/(k+1), mape_test/(k+1)\n",
        "        # print(\"mae_loss, rmse_loss, mape_loss \", mae_loss, rmse_loss, mape_loss)\n",
        "\n",
        "    val_loss.append(avg_valid_ls)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "    \n",
        "      print(\"epoch: {} | trn_loss: {:.8f} val_loss: {:.8f} \".format(epoch, trn_loss[-1], val_loss[-1]))\n",
        "\n",
        "end = time.time()\n",
        "print('time %.2f sec' % (end-start))\n",
        "print(\"*\"*100)\n",
        "\n",
        "print(\"mae_loss, rmse_loss, mape_loss: \", mae_loss, rmse_loss, mape_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "ahmT6h5cfjob",
        "outputId": "f3e805c6-aced-46f3-c183-0cde101e6d01"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(trn_loss, label = \"Training Loss\")\n",
        "plt.plot(val_loss, label = \"Validating Loss\")\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKSrgileaXlJ"
      },
      "source": [
        "## 30 Minutes Horizon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmXB30qmwCW1"
      },
      "outputs": [],
      "source": [
        "num_corridor =  num_corridor       # number of corridors also the channel in NTC\n",
        "hidden_size = 120        # lstm hidden_dim\n",
        "num_layers = 1          # lstm layers\n",
        "attention_size = 300     # the hidden_units of attention layer\n",
        "natt_hops = 2\n",
        "nfc = 512               # fully connected layer\n",
        "drop_prob = 0.2         # fully connected layer\n",
        "batch_size = 50\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = HierLstmat(num_corridor, hidden_size, num_layers, attention_size, natt_hops, nfc)\n",
        "model = model.to(device)\n",
        "\n",
        "learning_rate =  0.001\n",
        "\n",
        "num_epochs = 1000\n",
        "\n",
        "criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayKZApU4Fd_F",
        "outputId": "097400fc-c983-4713-c243-e1c8a0fe7258"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 10 | trn_loss: 0.67987754 val_loss: 0.69095455 \n",
            "epoch: 20 | trn_loss: 0.54920620 val_loss: 0.60607846 \n",
            "epoch: 30 | trn_loss: 0.47895866 val_loss: 0.54790625 \n",
            "epoch: 40 | trn_loss: 0.43328254 val_loss: 0.47655318 \n",
            "epoch: 50 | trn_loss: 0.40335637 val_loss: 0.44631645 \n",
            "epoch: 60 | trn_loss: 0.38267163 val_loss: 0.41354384 \n",
            "epoch: 70 | trn_loss: 0.36086519 val_loss: 0.40542271 \n",
            "epoch: 80 | trn_loss: 0.34249171 val_loss: 0.39724409 \n",
            "epoch: 90 | trn_loss: 0.32404052 val_loss: 0.39004882 \n",
            "epoch: 100 | trn_loss: 0.31392925 val_loss: 0.38024539 \n",
            "epoch: 110 | trn_loss: 0.30519756 val_loss: 0.38830791 \n",
            "epoch: 120 | trn_loss: 0.28920365 val_loss: 0.39656834 \n",
            "epoch: 130 | trn_loss: 0.26818865 val_loss: 0.37748452 \n",
            "epoch: 140 | trn_loss: 0.26426864 val_loss: 0.36427024 \n",
            "epoch: 150 | trn_loss: 0.24852770 val_loss: 0.35401597 \n",
            "epoch: 160 | trn_loss: 0.23680492 val_loss: 0.35648215 \n",
            "epoch: 170 | trn_loss: 0.23342118 val_loss: 0.36578849 \n",
            "epoch: 180 | trn_loss: 0.22188245 val_loss: 0.37604031 \n",
            "epoch: 190 | trn_loss: 0.22136000 val_loss: 0.37187004 \n",
            "epoch: 200 | trn_loss: 0.21360077 val_loss: 0.39404667 \n",
            "epoch: 210 | trn_loss: 0.19369388 val_loss: 0.38963404 \n",
            "epoch: 220 | trn_loss: 0.20226225 val_loss: 0.34648850 \n",
            "epoch: 230 | trn_loss: 0.18459065 val_loss: 0.34730878 \n",
            "epoch: 240 | trn_loss: 0.19024346 val_loss: 0.35289199 \n",
            "epoch: 250 | trn_loss: 0.18618693 val_loss: 0.35815577 \n",
            "epoch: 260 | trn_loss: 0.16866609 val_loss: 0.36926700 \n",
            "epoch: 270 | trn_loss: 0.17926635 val_loss: 0.36512989 \n",
            "epoch: 280 | trn_loss: 0.16993569 val_loss: 0.38412242 \n",
            "epoch: 290 | trn_loss: 0.15708969 val_loss: 0.33613261 \n",
            "epoch: 300 | trn_loss: 0.15912399 val_loss: 0.33048006 \n",
            "epoch: 310 | trn_loss: 0.16006138 val_loss: 0.33772955 \n",
            "epoch: 320 | trn_loss: 0.16025054 val_loss: 0.33280543 \n",
            "epoch: 330 | trn_loss: 0.17373532 val_loss: 0.34571558 \n",
            "epoch: 340 | trn_loss: 0.14718428 val_loss: 0.33863911 \n",
            "epoch: 350 | trn_loss: 0.15391156 val_loss: 0.35641158 \n",
            "epoch: 360 | trn_loss: 0.27345259 val_loss: 0.37401434 \n",
            "epoch: 370 | trn_loss: 0.14041229 val_loss: 0.33079750 \n",
            "epoch: 380 | trn_loss: 0.14334984 val_loss: 0.32471127 \n",
            "epoch: 390 | trn_loss: 0.13904783 val_loss: 0.32873240 \n",
            "epoch: 400 | trn_loss: 0.13737564 val_loss: 0.34262055 \n",
            "epoch: 410 | trn_loss: 0.14256181 val_loss: 0.36467701 \n",
            "epoch: 420 | trn_loss: 0.13559162 val_loss: 0.34391560 \n",
            "epoch: 430 | trn_loss: 0.13608023 val_loss: 0.34220095 \n",
            "epoch: 440 | trn_loss: 0.13005919 val_loss: 0.36373828 \n",
            "epoch: 450 | trn_loss: 0.13293619 val_loss: 0.36164241 \n",
            "epoch: 460 | trn_loss: 0.12501091 val_loss: 0.33351958 \n",
            "epoch: 470 | trn_loss: 0.12737718 val_loss: 0.32449370 \n",
            "epoch: 480 | trn_loss: 0.11616277 val_loss: 0.32341754 \n",
            "epoch: 490 | trn_loss: 0.11593703 val_loss: 0.32638538 \n",
            "epoch: 500 | trn_loss: 0.11746887 val_loss: 0.32807567 \n",
            "epoch: 510 | trn_loss: 0.12641263 val_loss: 0.32925659 \n",
            "epoch: 520 | trn_loss: 0.11301280 val_loss: 0.31598431 \n",
            "epoch: 530 | trn_loss: 0.11450048 val_loss: 0.32003708 \n",
            "epoch: 540 | trn_loss: 0.11639949 val_loss: 0.32743210 \n",
            "epoch: 550 | trn_loss: 0.11648430 val_loss: 0.33869655 \n",
            "epoch: 560 | trn_loss: 0.11230546 val_loss: 0.33751648 \n",
            "epoch: 570 | trn_loss: 0.11213691 val_loss: 0.33080776 \n",
            "epoch: 580 | trn_loss: 0.10582736 val_loss: 0.31209160 \n",
            "epoch: 590 | trn_loss: 0.10568132 val_loss: 0.33017931 \n",
            "epoch: 600 | trn_loss: 0.11018447 val_loss: 0.31347485 \n",
            "epoch: 610 | trn_loss: 0.11141634 val_loss: 0.32745745 \n",
            "epoch: 620 | trn_loss: 0.10397312 val_loss: 0.31295825 \n",
            "epoch: 630 | trn_loss: 0.10138145 val_loss: 0.30790132 \n",
            "epoch: 640 | trn_loss: 0.12858827 val_loss: 0.31130088 \n",
            "epoch: 650 | trn_loss: 0.09967755 val_loss: 0.31000704 \n",
            "epoch: 660 | trn_loss: 0.10403870 val_loss: 0.30422906 \n",
            "epoch: 670 | trn_loss: 0.09591800 val_loss: 0.30482046 \n",
            "epoch: 680 | trn_loss: 0.09825429 val_loss: 0.31413607 \n",
            "epoch: 690 | trn_loss: 0.09757191 val_loss: 0.31120410 \n",
            "epoch: 700 | trn_loss: 0.09792647 val_loss: 0.29921593 \n",
            "epoch: 710 | trn_loss: 0.10606075 val_loss: 0.34320949 \n",
            "epoch: 720 | trn_loss: 0.09642320 val_loss: 0.30410791 \n",
            "epoch: 730 | trn_loss: 0.09713164 val_loss: 0.31045780 \n",
            "epoch: 740 | trn_loss: 0.09583486 val_loss: 0.31193864 \n",
            "epoch: 750 | trn_loss: 0.09509530 val_loss: 0.31326547 \n",
            "epoch: 760 | trn_loss: 0.09534315 val_loss: 0.32133328 \n",
            "epoch: 770 | trn_loss: 0.09646717 val_loss: 0.30852191 \n",
            "epoch: 780 | trn_loss: 0.08942173 val_loss: 0.29958781 \n",
            "epoch: 790 | trn_loss: 0.08739393 val_loss: 0.30133325 \n",
            "epoch: 800 | trn_loss: 0.08842444 val_loss: 0.30547181 \n",
            "epoch: 810 | trn_loss: 0.09175453 val_loss: 0.30402387 \n",
            "epoch: 820 | trn_loss: 0.09011733 val_loss: 0.31758390 \n",
            "epoch: 830 | trn_loss: 0.08914425 val_loss: 0.32034720 \n",
            "epoch: 840 | trn_loss: 0.08950912 val_loss: 0.31211392 \n",
            "epoch: 850 | trn_loss: 0.08819330 val_loss: 0.30542203 \n",
            "epoch: 860 | trn_loss: 0.08976861 val_loss: 0.31180083 \n",
            "epoch: 870 | trn_loss: 0.08802080 val_loss: 0.31602782 \n",
            "epoch: 880 | trn_loss: 0.08842784 val_loss: 0.31277501 \n",
            "epoch: 890 | trn_loss: 0.08616335 val_loss: 0.31145351 \n",
            "epoch: 900 | trn_loss: 0.08614596 val_loss: 0.32035413 \n",
            "epoch: 910 | trn_loss: 0.08489308 val_loss: 0.31455241 \n",
            "epoch: 920 | trn_loss: 0.08534209 val_loss: 0.30851265 \n",
            "epoch: 930 | trn_loss: 0.08332788 val_loss: 0.30974338 \n",
            "epoch: 940 | trn_loss: 0.08523983 val_loss: 0.31874370 \n",
            "epoch: 950 | trn_loss: 0.08489078 val_loss: 0.31769740 \n",
            "epoch: 960 | trn_loss: 0.08377697 val_loss: 0.31071517 \n",
            "epoch: 970 | trn_loss: 0.07998463 val_loss: 0.31080059 \n",
            "epoch: 980 | trn_loss: 0.08529394 val_loss: 0.31267009 \n",
            "epoch: 990 | trn_loss: 0.08124745 val_loss: 0.30666758 \n",
            "epoch: 1000 | trn_loss: 0.08259340 val_loss: 0.30830695 \n",
            "time 7017.56 sec\n",
            "****************************************************************************************************\n",
            "mae_loss, rmse_loss, mape_loss:  0.23468431532382966 0.42408991008996966 0.0219551645219326\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, mean_absolute_error\n",
        "\n",
        "trainX = Variable(torch.Tensor(np.array(trn_x_30min)))\n",
        "trainY = Variable(torch.Tensor(np.array(trn_y_30min)))\n",
        "\n",
        "vldtX = Variable(torch.Tensor(np.array(vld_x_30min)))\n",
        "vldtY = Variable(torch.Tensor(np.array(vld_y_30min)))\n",
        "\n",
        "testX = Variable(torch.Tensor(np.array(tst_x_30min)))\n",
        "testY = Variable(torch.Tensor(np.array(tst_y_30min)))\n",
        "\n",
        "train_dataset = MyData(trainX, trainY)\n",
        "val_dataset = MyData(vldtX, vldtY)\n",
        "tst_dataset = MyData(testX, testY)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = batch_size)\n",
        "val_loader = DataLoader(val_dataset, batch_size = batch_size)\n",
        "tst_dataset = DataLoader(tst_dataset, batch_size = batch_size)\n",
        "\n",
        "trn_loss, val_loss = [], []\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "\n",
        "    ls = 0\n",
        "    valid_ls = 0\n",
        "\n",
        "    for i, train_batch in enumerate(train_loader):\n",
        "        inputs, targets = train_batch\n",
        "\n",
        "        # Move tensors to the configured device\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        model.train()\n",
        "        y_pred = model(inputs)\n",
        "        loss = criterion(y_pred, targets)\n",
        "\n",
        "        ls += loss.item()\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    avg_ls = ls/(i + 1)\n",
        "    trn_loss.append(avg_ls)\n",
        "\n",
        "    # Validating the model with current parameters\n",
        "\n",
        "    for j, val_batch in enumerate(val_loader):\n",
        "        inputs, targets = val_batch\n",
        "\n",
        "        # Move tensors to the configured device\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        model.eval()\n",
        "        y_pred = model(inputs)\n",
        "        loss = criterion(y_pred, targets)\n",
        "        valid_ls += loss.item()\n",
        "\n",
        "    avg_valid_ls = valid_ls/(j + 1)\n",
        "\n",
        "    if  len(val_loss) == 0 or avg_valid_ls < min(val_loss):\n",
        "\n",
        "        filepath = f'/content/drive/MyDrive/CIS 545 Project Folder/Data Set/hiLSTMat_30min_epoch_{epoch}_loss_{avg_valid_ls}.pth'\n",
        "\n",
        "        torch.save(model.state_dict(), filepath)\n",
        "        \n",
        "        mae_test,  rmse_test,  mape_test = 0, 0, 0\n",
        "\n",
        "        for k, tst_batch in enumerate(tst_dataset):\n",
        "\n",
        "            inputs, targets = tst_batch\n",
        "\n",
        "            # Move tensors to the configured device\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            model.eval()\n",
        "            y_pred = model(inputs)\n",
        "\n",
        "            y_pred = y_pred.cpu().detach().numpy()\n",
        "            targets = targets.cpu().detach().numpy()\n",
        "\n",
        "            # y_pred = scaler.inverse_transform(y_pred)\n",
        "            # targets = scaler.inverse_transform(targets)   \n",
        "\n",
        "            mae_test += mean_absolute_error(targets, y_pred)\n",
        "            \n",
        "            rmse_test += mean_squared_error(targets, y_pred, squared=False)\n",
        "\n",
        "            mape_test += mean_absolute_percentage_error(targets, y_pred)\n",
        "\n",
        "        mae_loss, rmse_loss, mape_loss = mae_test/(k+1), rmse_test/(k+1), mape_test/(k+1)\n",
        "        # print(\"mae_loss, rmse_loss, mape_loss \", mae_loss, rmse_loss, mape_loss)\n",
        "\n",
        "    val_loss.append(avg_valid_ls)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "    \n",
        "      print(\"epoch: {} | trn_loss: {:.8f} val_loss: {:.8f} \".format(epoch, trn_loss[-1], val_loss[-1]))\n",
        "\n",
        "end = time.time()\n",
        "print('time %.2f sec' % (end-start))\n",
        "print(\"*\"*100)\n",
        "\n",
        "print(\"mae_loss, rmse_loss, mape_loss: \", mae_loss, rmse_loss, mape_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "4nJ-aWNPbQPq",
        "outputId": "84da7fb5-18ae-4ed2-9262-ae7aca3e8a75"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(trn_loss, label = \"Training Loss\")\n",
        "plt.plot(val_loss, label = \"Validating Loss\")\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD6UIgmlacGt"
      },
      "source": [
        "## 45 minute Horizon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yV0hRW6YwIpo"
      },
      "outputs": [],
      "source": [
        "num_corridor =  num_corridor       # number of corridors also the channel in NTC\n",
        "hidden_size = 120        # lstm hidden_dim\n",
        "num_layers = 1          # lstm layers\n",
        "attention_size = 300     # the hidden_units of attention layer\n",
        "natt_hops = 2\n",
        "nfc = 512               # fully connected layer\n",
        "drop_prob = 0.5         # fully connected layer\n",
        "batch_size = 50\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = HierLstmat(num_corridor, hidden_size, num_layers, attention_size, natt_hops, nfc)\n",
        "model = model.to(device)\n",
        "\n",
        "learning_rate =  0.001\n",
        "\n",
        "num_epochs = 1000\n",
        "\n",
        "criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3CR_ZyjMX_e",
        "outputId": "4f8d8060-c2aa-43cf-a695-35c85ad3f747"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 10 | trn_loss: 0.86623535 val_loss: 0.83053108 \n",
            "epoch: 20 | trn_loss: 0.70471843 val_loss: 0.68247487 \n",
            "epoch: 30 | trn_loss: 0.61728253 val_loss: 0.59221025 \n",
            "epoch: 40 | trn_loss: 0.57283905 val_loss: 0.55359078 \n",
            "epoch: 50 | trn_loss: 0.53640685 val_loss: 0.52106260 \n",
            "epoch: 60 | trn_loss: 0.50639157 val_loss: 0.51090041 \n",
            "epoch: 70 | trn_loss: 0.48333296 val_loss: 0.49432261 \n",
            "epoch: 80 | trn_loss: 0.46115654 val_loss: 0.48036475 \n",
            "epoch: 90 | trn_loss: 0.44308276 val_loss: 0.46537382 \n",
            "epoch: 100 | trn_loss: 0.42617242 val_loss: 0.46970389 \n",
            "epoch: 110 | trn_loss: 0.41246965 val_loss: 0.48940318 \n",
            "epoch: 120 | trn_loss: 0.39921718 val_loss: 0.49313149 \n",
            "epoch: 130 | trn_loss: 0.38971182 val_loss: 0.45951189 \n",
            "epoch: 140 | trn_loss: 0.36882341 val_loss: 0.44243970 \n",
            "epoch: 150 | trn_loss: 0.35380137 val_loss: 0.44468863 \n",
            "epoch: 160 | trn_loss: 0.34106812 val_loss: 0.43688206 \n",
            "epoch: 170 | trn_loss: 0.32585025 val_loss: 0.45182191 \n",
            "epoch: 180 | trn_loss: 0.33111344 val_loss: 0.47084776 \n",
            "epoch: 190 | trn_loss: 0.32303745 val_loss: 0.48240712 \n",
            "epoch: 200 | trn_loss: 0.37345119 val_loss: 0.46515571 \n",
            "epoch: 210 | trn_loss: 0.30364242 val_loss: 0.46620840 \n",
            "epoch: 220 | trn_loss: 0.31063913 val_loss: 0.48357669 \n",
            "epoch: 230 | trn_loss: 0.29281008 val_loss: 0.47611304 \n",
            "epoch: 240 | trn_loss: 0.29029434 val_loss: 0.45102780 \n",
            "epoch: 250 | trn_loss: 0.28258285 val_loss: 0.44953011 \n",
            "epoch: 260 | trn_loss: 0.27337861 val_loss: 0.44972188 \n",
            "epoch: 270 | trn_loss: 0.27516024 val_loss: 0.43352195 \n",
            "epoch: 280 | trn_loss: 0.26722012 val_loss: 0.44760801 \n",
            "epoch: 290 | trn_loss: 0.27209782 val_loss: 0.46890386 \n",
            "epoch: 300 | trn_loss: 0.25564319 val_loss: 0.42154099 \n",
            "epoch: 310 | trn_loss: 0.25341154 val_loss: 0.41931807 \n",
            "epoch: 320 | trn_loss: 0.24393132 val_loss: 0.43282491 \n",
            "epoch: 330 | trn_loss: 0.24466732 val_loss: 0.44686467 \n",
            "epoch: 340 | trn_loss: 0.25241814 val_loss: 0.43378898 \n",
            "epoch: 350 | trn_loss: 0.23738837 val_loss: 0.43712628 \n",
            "epoch: 360 | trn_loss: 0.23675006 val_loss: 0.42021888 \n",
            "epoch: 370 | trn_loss: 0.26130675 val_loss: 0.42031098 \n",
            "epoch: 380 | trn_loss: 0.22840114 val_loss: 0.41558284 \n",
            "epoch: 390 | trn_loss: 0.21974322 val_loss: 0.41529538 \n",
            "epoch: 400 | trn_loss: 0.21348411 val_loss: 0.40410315 \n",
            "epoch: 410 | trn_loss: 0.21160120 val_loss: 0.41068375 \n",
            "epoch: 420 | trn_loss: 0.20841131 val_loss: 0.41535727 \n",
            "epoch: 430 | trn_loss: 0.22152708 val_loss: 0.41164678 \n",
            "epoch: 440 | trn_loss: 0.20190047 val_loss: 0.40921348 \n",
            "epoch: 450 | trn_loss: 0.20321324 val_loss: 0.41259292 \n",
            "epoch: 460 | trn_loss: 0.19317290 val_loss: 0.43038732 \n",
            "epoch: 470 | trn_loss: 0.19266761 val_loss: 0.40916249 \n",
            "epoch: 480 | trn_loss: 0.19771345 val_loss: 0.41125661 \n",
            "epoch: 490 | trn_loss: 0.20047801 val_loss: 0.41435541 \n",
            "epoch: 500 | trn_loss: 0.19165981 val_loss: 0.40273322 \n",
            "epoch: 510 | trn_loss: 0.18018827 val_loss: 0.40907868 \n",
            "epoch: 520 | trn_loss: 0.18093077 val_loss: 0.40194617 \n",
            "epoch: 530 | trn_loss: 0.17863948 val_loss: 0.41193547 \n",
            "epoch: 540 | trn_loss: 0.18157477 val_loss: 0.41039063 \n",
            "epoch: 550 | trn_loss: 0.17926557 val_loss: 0.40494706 \n",
            "epoch: 560 | trn_loss: 0.17972799 val_loss: 0.42179164 \n",
            "epoch: 570 | trn_loss: 0.17538116 val_loss: 0.39505930 \n",
            "epoch: 580 | trn_loss: 0.18282152 val_loss: 0.39468785 \n",
            "epoch: 590 | trn_loss: 0.17719112 val_loss: 0.39093315 \n",
            "epoch: 600 | trn_loss: 0.16548929 val_loss: 0.38092921 \n",
            "epoch: 610 | trn_loss: 0.16229542 val_loss: 0.38533590 \n",
            "epoch: 620 | trn_loss: 0.16495013 val_loss: 0.39733286 \n",
            "epoch: 630 | trn_loss: 0.16767935 val_loss: 0.37732669 \n",
            "epoch: 640 | trn_loss: 0.16726669 val_loss: 0.38988645 \n",
            "epoch: 650 | trn_loss: 0.17731394 val_loss: 0.38748113 \n",
            "epoch: 660 | trn_loss: 0.15880365 val_loss: 0.38838176 \n",
            "epoch: 670 | trn_loss: 0.21898637 val_loss: 0.38819968 \n",
            "epoch: 680 | trn_loss: 0.15813064 val_loss: 0.38442230 \n",
            "epoch: 690 | trn_loss: 0.22257354 val_loss: 0.40811260 \n",
            "epoch: 700 | trn_loss: 0.15336783 val_loss: 0.39174945 \n",
            "epoch: 710 | trn_loss: 0.15391030 val_loss: 0.39202985 \n",
            "epoch: 720 | trn_loss: 0.14941365 val_loss: 0.37893579 \n",
            "epoch: 730 | trn_loss: 0.16585089 val_loss: 0.39641322 \n",
            "epoch: 740 | trn_loss: 0.15038452 val_loss: 0.38906241 \n",
            "epoch: 750 | trn_loss: 0.15854421 val_loss: 0.38670952 \n",
            "epoch: 760 | trn_loss: 0.16055072 val_loss: 0.40170303 \n",
            "epoch: 770 | trn_loss: 0.14163349 val_loss: 0.38590902 \n",
            "epoch: 780 | trn_loss: 0.15215630 val_loss: 0.39405266 \n",
            "epoch: 790 | trn_loss: 0.14504365 val_loss: 0.39671470 \n",
            "epoch: 800 | trn_loss: 0.15210600 val_loss: 0.40941776 \n",
            "epoch: 810 | trn_loss: 0.14391176 val_loss: 0.40618659 \n",
            "epoch: 820 | trn_loss: 0.14109112 val_loss: 0.39947907 \n",
            "epoch: 830 | trn_loss: 0.19812055 val_loss: 0.42415309 \n",
            "epoch: 840 | trn_loss: 0.14907142 val_loss: 0.39832586 \n",
            "epoch: 850 | trn_loss: 0.14807216 val_loss: 0.41583455 \n",
            "epoch: 860 | trn_loss: 0.13668654 val_loss: 0.42044390 \n",
            "epoch: 870 | trn_loss: 0.14567849 val_loss: 0.40109972 \n",
            "epoch: 880 | trn_loss: 0.14270412 val_loss: 0.39665196 \n",
            "epoch: 890 | trn_loss: 0.14381623 val_loss: 0.40475271 \n",
            "epoch: 900 | trn_loss: 0.14078672 val_loss: 0.40280014 \n",
            "epoch: 910 | trn_loss: 0.14076898 val_loss: 0.40397532 \n",
            "epoch: 920 | trn_loss: 0.14994808 val_loss: 0.41922192 \n",
            "epoch: 930 | trn_loss: 0.13796935 val_loss: 0.41083897 \n",
            "epoch: 940 | trn_loss: 0.13595289 val_loss: 0.39683460 \n",
            "epoch: 950 | trn_loss: 0.13511318 val_loss: 0.40339413 \n",
            "epoch: 960 | trn_loss: 0.13504243 val_loss: 0.40124240 \n",
            "epoch: 970 | trn_loss: 0.14111205 val_loss: 0.40768266 \n",
            "epoch: 980 | trn_loss: 0.14131141 val_loss: 0.39181656 \n",
            "epoch: 990 | trn_loss: 0.14139389 val_loss: 0.39388771 \n",
            "epoch: 1000 | trn_loss: 0.13648711 val_loss: 0.40637979 \n",
            "time 6837.04 sec\n",
            "****************************************************************************************************\n",
            "Best Model mae_loss, rmse_loss, mape_loss:  0.2689810825511813 0.4907733216881752 0.025177316600456835\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, mean_absolute_error\n",
        "\n",
        "trainX = Variable(torch.Tensor(np.array(trn_x_45min)))\n",
        "trainY = Variable(torch.Tensor(np.array(trn_y_45min)))\n",
        "\n",
        "vldtX = Variable(torch.Tensor(np.array(vld_x_45min)))\n",
        "vldtY = Variable(torch.Tensor(np.array(vld_y_45min)))\n",
        "\n",
        "testX = Variable(torch.Tensor(np.array(tst_x_45min)))\n",
        "testY = Variable(torch.Tensor(np.array(tst_y_45min)))\n",
        "\n",
        "train_dataset = MyData(trainX, trainY)\n",
        "val_dataset = MyData(vldtX, vldtY)\n",
        "tst_dataset = MyData(testX, testY)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = batch_size)\n",
        "val_loader = DataLoader(val_dataset, batch_size = batch_size)\n",
        "tst_dataset = DataLoader(tst_dataset, batch_size = batch_size)\n",
        "\n",
        "trn_loss, val_loss = [], []\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "\n",
        "    ls = 0\n",
        "    valid_ls = 0\n",
        "\n",
        "    for i, train_batch in enumerate(train_loader):\n",
        "        inputs, targets = train_batch\n",
        "\n",
        "        # Move tensors to the configured device\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        model.train()\n",
        "        y_pred = model(inputs)\n",
        "        loss = criterion(y_pred, targets)\n",
        "\n",
        "        ls += loss.item()\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    avg_ls = ls/(i + 1)\n",
        "    trn_loss.append(avg_ls)\n",
        "\n",
        "    # Validating the model with current parameters\n",
        "\n",
        "    for j, val_batch in enumerate(val_loader):\n",
        "        inputs, targets = val_batch\n",
        "\n",
        "        # Move tensors to the configured device\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        model.eval()\n",
        "        y_pred = model(inputs)\n",
        "        loss = criterion(y_pred, targets)\n",
        "        valid_ls += loss.item()\n",
        "\n",
        "    avg_valid_ls = valid_ls/(j + 1)\n",
        "\n",
        "    if  len(val_loss) == 0 or avg_valid_ls < min(val_loss):\n",
        "\n",
        "        filepath = f'/content/drive/MyDrive/CIS 545 Project Folder/Data Set/hiLSTMat_45min_epoch_{epoch}_loss_{avg_valid_ls}.pth'\n",
        "\n",
        "        torch.save(model.state_dict(), filepath)\n",
        "        \n",
        "        mae_test,  rmse_test,  mape_test = 0, 0, 0\n",
        "\n",
        "        for k, tst_batch in enumerate(tst_dataset):\n",
        "\n",
        "            inputs, targets = tst_batch\n",
        "\n",
        "            # Move tensors to the configured device\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            model.eval()\n",
        "            y_pred = model(inputs)\n",
        "\n",
        "            y_pred = y_pred.cpu().detach().numpy()\n",
        "            targets = targets.cpu().detach().numpy()\n",
        "\n",
        "            # y_pred = scaler.inverse_transform(y_pred)\n",
        "            # targets = scaler.inverse_transform(targets)   \n",
        "\n",
        "            mae_test += mean_absolute_error(targets, y_pred)\n",
        "            \n",
        "            rmse_test += mean_squared_error(targets, y_pred, squared=False)\n",
        "\n",
        "            mape_test += mean_absolute_percentage_error(targets, y_pred)\n",
        "\n",
        "        mae_loss, rmse_loss, mape_loss = mae_test/(k+1), rmse_test/(k+1), mape_test/(k+1)\n",
        "        # print(\"mae_loss, rmse_loss, mape_loss \", mae_loss, rmse_loss, mape_loss)\n",
        "\n",
        "    val_loss.append(avg_valid_ls)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "    \n",
        "      print(\"epoch: {} | trn_loss: {:.8f} val_loss: {:.8f} \".format(epoch, trn_loss[-1], val_loss[-1]))\n",
        "\n",
        "end = time.time()\n",
        "print('time %.2f sec' % (end-start))\n",
        "print(\"*\"*100)\n",
        "\n",
        "print(\"Best Model mae_loss, rmse_loss, mape_loss: \", mae_loss, rmse_loss, mape_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5im__rxZbRmm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "ee6291ff-75d1-48a2-80e2-ff93c4306372"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(trn_loss, label = \"Training Loss\")\n",
        "plt.plot(val_loss, label = \"Validating Loss\")\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
